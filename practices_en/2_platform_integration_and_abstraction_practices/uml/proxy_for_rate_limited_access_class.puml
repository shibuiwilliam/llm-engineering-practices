@startuml Proxy for Rate-Limited Access Pattern - Class Diagram

skinparam classAttributeIconSize 0

class Client {
  + requestLLMResponse(prompt: String): Response
}

class LLMProxy {
  - rateLimiter: RateLimiter
  - cache: Cache
  - retryPolicy: RetryPolicy
  - queue: RequestQueue
  + handleRequest(request: Request): Response
  - throttleRequest(request: Request): void
  - retryWithBackoff(request: Request): Response
  - cacheResponse(request: Request, response: Response): void
  - logRequest(request: Request, response: Response): void
}

class RateLimiter {
  - limits: Map<String, Integer>
  + checkLimit(userId: String): boolean
  + updateLimit(userId: String): void
}

class Cache {
  - cacheStore: Map<String, Response>
  + get(key: String): Response
  + put(key: String, value: Response): void
}

class RetryPolicy {
  - maxRetries: int
  - backoffStrategy: BackoffStrategy
  + shouldRetry(error: Error): boolean
  + getNextRetryDelay(): Duration
}

class RequestQueue {
  - queue: Queue<Request>
  + enqueue(request: Request): void
  + dequeue(): Request
}

class LLMService {
  + processRequest(request: Request): Response
}

Client --> LLMProxy
LLMProxy --> RateLimiter
LLMProxy --> Cache
LLMProxy --> RetryPolicy
LLMProxy --> RequestQueue
LLMProxy --> LLMService

note right of LLMProxy
  Centralizes rate limiting,
  caching, retry logic, and
  request queuing
end note

note right of RateLimiter
  Manages per-user and
  global rate limits
end note

note right of Cache
  Stores responses for
  identical prompts
end note

note right of RetryPolicy
  Implements exponential
  backoff strategy
end note

@enduml 