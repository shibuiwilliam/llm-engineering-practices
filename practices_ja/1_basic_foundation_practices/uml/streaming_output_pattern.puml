@startuml streaming_output_pattern

actor User
participant "Client\n(Frontend)" as Client
participant "Backend Server" as Backend
participant "LLM Service" as LLM

== Initial Request ==
User -> Client: Submit prompt
Client -> Backend: Send prompt request
activate Backend
Backend -> LLM: Request with streaming=true
activate LLM

== Streaming Response ==
loop For each token
    LLM --> Backend: Stream token
    Backend --> Client: Push token via SSE/WebSocket
    Client --> User: Render token
end

== Error Handling ==
alt Error occurs
    LLM --> Backend: Error response
    Backend --> Client: Error notification
    Client --> User: Display error
end

== Cancellation ==
alt User cancels
    User -> Client: Cancel request
    Client -> Backend: Cancel request
    Backend -> LLM: Abort request
    LLM --> Backend: Acknowledge cancellation
    Backend --> Client: Confirm cancellation
    Client --> User: Update UI
end

== Completion ==
LLM --> Backend: Stream complete
deactivate LLM
Backend --> Client: Stream end notification
deactivate Backend
Client --> User: Finalize response

@enduml 