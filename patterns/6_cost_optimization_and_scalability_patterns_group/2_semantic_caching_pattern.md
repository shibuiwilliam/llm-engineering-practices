# Semantic Caching Pattern

## 概要
Semantic Caching Patternは、プロンプトの文字列が完全に一致していなくても意味的に同じと判断されるリクエストに対して、LLMの応答を再利用するための設計手法です。このパターンにより、キャッシュヒット率を向上させ、LLM APIのコストとレイテンシを大幅に削減することができます。特に自然言語で多様な入力がなされるシステムにおいて、高いキャッシュ効果を発揮します。

## 解決したい課題
従来の文字列ベースのキャッシュは、プロンプトが完全一致しなければキャッシュミスとして扱われるため、以下のような課題が発生します。

1. **わずかな表現の違いでキャッシュミスが発生**
   - 例：「東京の天気を教えて」と「今日の東京の天気を知りたい」は意味は同じですが、文字列が異なるため通常のキャッシュにはヒットしません。

2. **類似質問への再計算によるAPIコスト増大**
   - 例：FAQボットで「返品の方法を教えてください」と「返品できますか？」のような似た質問が大量に送られるたびにLLMが都度呼び出されます。

3. **キャッシュの冗長性**
   - 例：文字列がわずかに違うだけで異なる応答キャッシュが多数生成され、ストレージの効率が悪くなります。

## 解決策
Semantic Caching Patternでは、プロンプトの「意味的な類似性」に基づいてキャッシュを管理します。

1. **プロンプトの埋め込み（Embedding）を取得**
   - ユーザーの入力プロンプトをベクトル化して意味的な特徴を表現します。
   - 例：OpenAIのtext-embedding-ada-002や、Sentence Transformersなどのモデルを使用します。

2. **意味的に近いプロンプトとの類似度を計算**
   - 既存のキャッシュとベクトル距離で比較し、閾値以上の類似度があればそれを「キャッシュヒット」と見なします。
   - 例：コサイン類似度が0.95以上の場合にキャッシュヒットと判定します。

3. **キャッシュ応答の再利用**
   - 類似したプロンプトに対しては、意味的に対応可能な既存応答を返すようにします。
   - 例：キャッシュヒットした場合、元のプロンプトと応答のペアを記録し、類似度スコアとともに保存します。

## 適応するシーン
このパターンは以下のような場面で特に有効です。

- ユーザーからの問い合わせが自然言語で多様に表現されるチャットボットやFAQシステム
- 入力文のバリエーションが多いが、回答は定型的であるようなサポート業務
- LLMのAPIコスト削減が重要な運用環境
- 応答時間の短縮が求められる高トラフィックなシステム

## 利用するメリット
このパターンを採用することで、以下のメリットが得られます。

- キャッシュヒット率が向上し、LLM API呼び出し回数を大幅に削減できます。
- 応答時間が短縮され、ユーザー体験が向上します。
- 表現ゆれに強くなり、類似クエリに対して一貫性のある応答が得られます。
- キャッシュの冗長性が減少し、キャッシュストレージの効率が向上します。

## 注意点とトレードオフ
このパターンを採用する際は、以下の点に注意が必要です。

- 埋め込みモデルの品質に依存するため、精度の低いモデルでは誤判定が発生する可能性があります。
- 類似度閾値のチューニングが必要で、高すぎるとキャッシュヒットが減り、低すぎると誤応答が返る可能性があります。
- 埋め込みと検索の処理に追加の計算リソースが必要になります。
- キャッシュの有効期限や更新戦略の設計が重要になります。

## 導入のヒント
このパターンを効果的に導入するためのポイントは以下の通りです。

1. ベクトル検索用にFAISSやMilvusなどのベクトルDBを利用します。
2. 類似度の閾値はログを見ながら段階的に調整します。
3. 回答の正確性が重要な場面では、キャッシュヒット後にもLLMによる再検証を挟む設計も有効です。
4. キャッシュ内容には、元のプロンプトとキャッシュヒット元のプロンプトを記録しておくとトラブルシュートに役立ちます。

## まとめ
Semantic Caching Patternは、意味的に類似したプロンプトに対して同じ応答を再利用することで、LLMのAPIコスト削減と応答速度向上を両立させる有効な手法です。特に自然言語で多様な入力がなされるシステムにおいて、高いキャッシュ効果を発揮します。意味理解に基づくキャッシュ戦略として、本番運用時の最適化に大きく貢献します。
