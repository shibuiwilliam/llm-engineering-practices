# Context Compression & Pruning Pattern

## 説明  
Context Compression & Pruning Pattern は、LLM に渡すプロンプト内の「過去コンテキスト」や「会話履歴」を**要約・圧縮**し、さらに関連度の低い情報を**削減（プルーニング）**して、トークン消費量を最小化すると同時に必須情報のみを保持する設計手法です。  
- **圧縮フェーズ**：長文履歴を要約モデルやヒューリスティックで短縮  
- **関連度スコアリング**：各発話やドキュメント断片に対して現在のクエリとの関連度をスコア化  
- **プルーニングフェーズ**：スコア閾値未満の履歴を切り捨て  
- **ローカルキャッシュ併用**：圧縮前のフル履歴は低コストストアに保持し、必要時に再生成可能  

## 用途  
- **マルチターンチャット**：長時間対話で肥大化した履歴を要約・剪定し、プロンプト長制限内に収める  
- **RAG 対話型検索**：検索コンテキストを圧縮して再検索トークンを節約  
- **ドキュメントレビュー支援**：大量コメント・注釈を要約し、レビュー時に重要箇所のみを提示  
- **エージェント型ワークフロー**：複数ステップの中間結果を圧縮し、次ステップへ効率的に引き継ぐ  

## 解決する課題  
1. **プロンプト長制限超過**  
   - 全履歴を毎回添付するとトークン上限に達し、対話が途切れる  
2. **無駄なトークン消費**  
   - 既に不要になった過去情報まで毎回送信し、コストと遅延を増大  
3. **関連性低下**  
   - 古い履歴や冗長情報が混在すると、モデルの焦点が分散して品質が低下  
4. **メモリ肥大**  
   - 長期記憶として保持するデータ量が増え、検索・伝送コストが増大  

## 対象とするシステム／プロジェクト  
- **カスタマーサポートチャット**：過去 100 ターンを要約し、最新の 5 ターン＋要約履歴のみをプロンプトに含む  
- **コーディングアシスタント**：クラス全体の履歴ではなく、直近編集箇所と要約ドキュメントを維持  
- **法律文書レビュー**：大量のコメント欄を要約し、最新提案や重要判例のみを抜粋  
- **マルチエージェントオーケストレーション**：各エージェント会話を圧縮して中央コンテキストに集約  

## 利用するメリット  
- **トークン節約**：要約・剪定で送信トークン量を最大 70–90% 削減可能  
- **応答速度向上**：短いプロンプトで高速にモデル呼び出し  
- **品質維持**：関連情報のみ残すことで、モデルの注意力を最適化  
- **スケーラビリティ**：長期対話や大規模レビューでも定常的にプロンプト長を制御  

## 注意点とトレードオフ  
- **要約品質依存**  
  - 圧縮要約モデルの精度が低いと、重要情報を誤って削除するリスク  
- **処理コスト増**  
  - 圧縮・関連度計算フェーズが追加され、前処理コストやシステム複雑度が増大  
- **履歴再現要件**  
  - フル履歴再生成が必要な場合、ローカルキャッシュや別ストア設計が必須  
- **チューニング負荷**  
  - 閾値や要約長、関連度スコア基準の最適化に継続的な評価・調整が必要  

## 導入のヒント  
1. **ヒストリープロファイリング**  
   - 初期は過去履歴の使用頻度と関連度を分析し、要約／削除対象を特定  
2. **フェーズ分離**  
   - 圧縮要約処理とプルーニング処理を分離し、段階的に導入・評価  
3. **ハイブリッド要約**  
   - LLM 要約＋ルールベース抽出（見出し、キーワード）の併用で精度とコストを両立  
4. **キャッシュ＆フォールバック**  
   - フル履歴はローカルキャッシュに保持し、要約生成エラー時は元データにフォールバック  
5. **モニタリングと A/B テスト**  
   - 圧縮前後のモデル応答品質、トークン削減率、応答レイテンシを継続的に比較評価  
